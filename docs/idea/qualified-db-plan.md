# Qualified Database Plan

## Problem

We cannot have a fixed structure database at gather time because we don't know the exact nature of what we're collecting. The gather phase is exploratory.

## Solution: Two-Database Architecture

### Database 1: Gather (Unqualified)
- **Name**: `gather_[project-slug]`
- **Purpose**: Raw, unstructured data collection
- **Status**: Exploratory, may have duplicates, contradictions, incomplete info
- **Collections**: Flexible - anything the user or AI generates during exploration

### Database 2: Qualified (Production-Ready)
- **Name**: `qualified_[project-slug]`
- **Purpose**: Evaluated, validated, production-ready data
- **Status**: Clean, deduplicated, consistent, passed evaluation
- **Collections**: Structured data ready for script and asset generation

## Data Flow

```
User Input → Gather DB (unqualified)
              ↓
         Evaluation System
              ↓
         Brain Validation
              ↓
    Qualified DB (production-ready) ← Only if passes
              ↓
    Script Generation
              ↓
    Asset Generation (videos, images)
              ↓
         Final Product
```

## Database Naming Pattern

```
Project: "aladdin-2024"

Gather:     gather_aladdin-2024
Qualified:  qualified_aladdin-2024
```

## Next Steps

- Design migration system: gather → qualified
- Define what "qualified" means per collection type
- Create script generation that uses qualified data
- Build asset generation pipeline

---

## Detailed Workflow Design

### Phase 1: Unqualified Data Collection (Gather)
- User explores ideas in `gather_[project-slug]` database
- Raw, flexible data - no structure requirements
- Evaluation system determines readiness

### Phase 2: Qualification Process (After Project Readiness Complete)

**Trigger**: User presses button after project readiness is complete

**Orchestrator runs through all departments in parallel:**
1. Story Department → Generates screenplay broken into scenes
2. Character Department → Character profiles and relationships
3. Visual Department → Visual style, mood, cinematography guidelines
4. World Agents Department → Story bible verification
5. Other departments → Their respective outputs

**Outputs:**
- Qualified data written to:
  - PayloadCMS (structured, known formats)
  - `qualified_[project-slug]` database (flexible collections)
- Brain ingestion (all qualified data validated and embedded)

**Story Bible:**
- Question: How does story bible fit? Is it generated during qualification or exists in gather?
- Verification system needed for consistency checking
- All qualified data must be story-bible verified

### Phase 3: Scene Structure & Constraints

**Key Constraint**: Each scene = 7 seconds max video

**Scene Collection Structure** (PayloadCMS):
- Extensive structure covering all aspects of a scene
- **No required fields** - everything optional
- Users can update/improve any aspect via PayloadCMS UI
- Fields include everything that can describe a scene:
  - Dialogue
  - Camera angles/shots
  - Character positions/blocking
  - Lighting/mood/atmosphere
  - Props/costumes
  - Audio cues (music, SFX)
  - Transitions
  - Visual style
  - Prompts for generation
  - Base information for prompts
  - [More to be defined]
- Each field has description
- Optional `generatedByAgent` relationship field for prompt optimization

**Screenplay:**
- Full screenplay generated by Story Department
- Broken down into 7-second scenes
- Each scene becomes structured document

### Phase 4: Media Generation (Scenes Only)

**Generation happens in PayloadCMS** (3rd phase after qualification)

**Two-Step Generation Process:**
1. **Scene Start Image Generation**
   - Input: Prompts + optional image
   - Output: Scene start image
   - Question: Do we always need this step or can we skip to video?

2. **Video Generation**
   - Input: Prompt + start image
   - Output: 7-second video
   - Question: Per scene or per shot within scene?
   - Question: Support multiple start images per scene?

**Verification System:**
- Vision model verification needed
- Image verification must plugin somehow
- Question: How does this integrate with story bible verification?

### Phase 5: Qualified Database Structure

**Database**: `qualified_[project-slug]`

**Collections**: Dynamic number, project-specific
- Collection names are descriptive
- All data stored as JSON key:value pairs
- Flexible schema per collection
- Examples:
  - `screenplay`
  - `story_bible`
  - `scene_metadata`
  - `character_profiles`
  - `visual_guidelines`
  - `world_rules`
  - [More as needed]

### Data Distribution After Qualification

**PayloadCMS** (Structured):
- Scenes collection (extensive structure, no required fields)
- Media collection (generated images/videos)
- Screenplay reference
- Agent executions

**Qualified Database** (Flexible):
- Dynamic collections as needed
- All JSON key:value
- Project-specific structure

**Brain** (Knowledge Graph):
- All qualified data ingested
- Embeddings generated
- Consistency validation
- Story bible enforcement

---

## Answers & Decisions

### 1. Story Bible Generation
- **When**: Automatically generated during qualification process
- **How**: Compiled from all qualified data (characters, world rules, visual guidelines, etc.)
- **Purpose**: Becomes the "source of truth" for all verification

### 2. Vision Model Verification Workflow
```
Scene Image Generated
     ↓
360° Generation Workflow → Multiple angles/views
     ↓
Brain Ingestion (all views)
     ↓
Vision Model Verification:
  - Query Brain: "Is the face of Jackson correct in this image?"
  - Check against character descriptions
  - Verify story bible rules
  - Validate visual style guide
     ↓
Pass → Save to PayloadCMS
Fail → Show error, stop (no fallbacks, no mocks - costs money)
```

**Key**: 360° generation fed into brain for comprehensive verification

### 3. Video Generation Methods & Scene Logic

**Three Generation Methods:**
1. **Text to Video** - Direct from prompt
2. **Image to Video** - Start image + prompt
3. **Video to Video** - Existing video transformation

**Scene Sequencing:**
- Multiple scenes combine to create sequences
- **Last Frame Service** (`last-frame.ft.tc`):
  - Extracts last frame from previous video
  - Can be used as start image for next scene
  - Or create custom transition frame as needed
  - Can pass last frame to "nano banana" as reference image
  - **Video Stitching Service**: Combines multiple videos in sequence into single video

**Documentation**: See `docs/last-frame/how-to-use.md`

**Workflow Example:**
```
Scene 1 → Generate Video (7s)
           ↓
      Extract Last Frame
           ↓
Scene 2 → Use as Start Image → Generate Video (7s)
           ↓
      Extract Last Frame
           ↓
Scene 3 → Use as Start Image → Generate Video (7s)
           ↓
      Video Stitching Service
           ↓
      Combined Sequence (21s)
```

### 4. Scene Collection Structure - Confirmed

**Categories to include:**
- **Story**: Dialogue, action, purpose, beat
- **Visual**: Camera, lighting, framing, composition, color palette
- **Audio**: Music, SFX, ambient sound, dialogue audio
- **Characters**: Who's present, emotions, positions, costumes
- **Props/Set**: What's in the scene, environment details
- **Technical**: Duration, transitions, special effects
- **Generation**: Text prompts, image prompts, model parameters, last frame reference
- **Metadata**: Generated by agent, version, approval status
- **All fields optional** - no required fields

### 5. Qualification Button Workflow

**Trigger**: User presses "Generate Qualified Data" button after project readiness complete

**Process** (Fully Automated with Edit Opportunities):
1. **Start**: Orchestrator launches all departments in parallel
2. **Progress**: Real-time progress shown per department
3. **Edit Opportunity**: Timed popup offers user chance to edit
   - If user clicks: Pause, allow edits, resume
   - If no interaction: Continue automatically after timeout
4. **Error Handling**:
   - **STOP IMMEDIATELY** on any error
   - Display error at top of all pages (global notification)
   - **NO FALLBACKS, NO MOCKS** - Generations cost money
   - User must fix error before proceeding
5. **Completion**: All qualified data written to PayloadCMS + qualified DB + Brain

**No Automatic Retry**: Every generation attempt costs money

---

## Resolved Questions

1. ✅ **Story Bible Generation**: Automatic during qualification
2. ✅ **Vision Model Verification**: 360° generation → Brain → Query verification
3. ✅ **Scene Image Generation**: Optional, supports last-frame extraction
4. ✅ **Generation Granularity**: Per scene (7s), combine with stitching service
5. ✅ **Multiple Start Images**: Supported via 360° workflow
6. ✅ **Verification Plugin**: Brain-based query system
7. ✅ **Scene Field Definitions**: Comprehensive structure confirmed
8. ✅ **Agent Optimization**: `generatedByAgent` field tracks optimization

---

## Remaining Questions - ANSWERED

### 1. Last Frame Service Integration ✅
**Decision**: Conditional automatic
- If Scene 2 **cannot** have different start image → Use Scene 1 last frame automatically
- Can also **prompt using last frame** as reference, but know it won't be perfect match
- **Shot Agent** makes this decision per scene

### 2. 360° Generation Trigger ✅
**When**: During **character profile generation** (qualification phase)

**Workflow**:
```
Character Identified
     ↓
Generate Character Profile
     ↓
Generate Master Reference from Profile
     ↓
Generate 360° Views from Master Reference
     ↓
Store all reference images in Brain
```

**Purpose**: Check mechanism for scene generation
- Each reference image is **highly descriptive**
- Different angles (front, back, side, etc.) available
- **Best reference image** retrieved based on scene needs
- Example: Character's back reference ≠ face reference

**Scene Generation Retrieval**:
- Agent can request: "Get me the best image of Jason from his back"
- Brain returns most appropriate reference from 360° set

### 3. Nano Banana Reference ✅
**Service**: https://fal.ai/models/fal-ai/nano-banana/edit/llms.txt
**Purpose**: Image editing/refinement using last frame as reference
**Usage**: Can pass last frame to Nano Banana for scene consistency

### 4. Video Stitching ✅
**Decision**: Automatic
- If scenes exist in sequence, send them automatically
- Stitching service combines them in order

### 5. Edit Popup Timing ✅
**Timeout**: 10 seconds
- After 10s, auto-continue if no user interaction

### 6. Brain Query Format ✅
**Who Writes**: Agents during verification process

**Query Examples**:
- "Is the face of Jackson correct in this image?"
- "Get me the best image of Jason from his back"
- Agents write queries as part of verification workflow
- Agents can also retrieve specific reference images for scene generation

---

## New Critical Questions

### Character Reference System

1. **Master Reference Generation**:
   - What model/service generates master reference from profile?
   env keys
     FAL_TEXT_TO_IMAGE_MODEL=fal-ai/nano-banana << this one generates since we are generating based on character profile which will be text
     

   - What prompt template is used?
   have a prompt template for this. it should be in appropriate collection in the characters collection so that we can enhance if required. as default ensure you have a default prompt template which is good. the main thing is to ensure we are generating high quality images and can generate image from it.
   - How detailed should the profile be before generating master reference?
   full profile should be generated before we generate the master reference.

2. **360° View Generation**:
   - How many angles in the 360° set? (8 angles? 16? Custom per character?)
   - Standard angles: Front, Back, Left Side, Right Side, 3/4 view and full view is enough. <<< this one. we call it 360 but it is not actually 360. >>> 
   - Are these generated simultaneously or sequentially? << simultaneously based on master, until unless you think otherwise>>
   - What service generates 360° views? (Fal.ai? Different model?) <<FAL_IMAGE_TO_IMAGE_MODEL=fal-ai/nano-banana/edit>>

3. **Reference Image Descriptions**:
   - Who writes the "highly descriptive" metadata for each reference image?
   - Is this done by an agent during generation? <<this>>
   - What metadata fields are stored? <<be as detailed as possible. better more than less>>
     - Angle/view (front, back, side)?
     - Visible features (face, hands, costume details)?
     - Lighting conditions?
     - Emotional state?
     - Other?

4. **Brain Storage for References**:
   - How are 360° references stored in Brain? <<images and descriptions. brain is multimodal does image and text work well. no video though>>
   - Embeddings for both image + description? <<brain handles it. check how to use documentation>>
   - How does retrieval work: "Get best image of Jason from his back" <<the brain should be able to give you the image name. we are using multi modal image embeddings. so we should be able to search for image based on text.>>
     - Vector similarity search on description?
     - Metadata filtering + similarity?
     - Agent interprets request and queries?

5. **Reference Image Usage in Scenes**:
   - When generating a scene, does the agent automatically retrieve appropriate references? <<should>>
   - Or does user manually select which reference to use? <<nothing manual>>
   - Can multiple references be used in one scene (e.g., Jason front + Sarah side)? <<absolutely. we need refernce for localtion, characters, props, etc.>>

### Shot Agent Decision Logic

6. **Shot Agent Responsibilities**:
   - What exactly does the "shot agent" decide? <<Given the scene, it decides what is the best way to generate the scene, refer to industry standards. something like .>>
   - When Scene 2 needs start image:
     - Evaluate: Can we use Scene 1 last frame << yes >>
     - Evaluate: Should we generate new start image << yes >>
     - Evaluate: Should we generate new start image and take the last frame image as reference << yes >>
     - Evaluate: Should we retrieve character reference <<you have to use all props and characters in the scene.>>
   - What criteria determine these decisions? <<the content of the scene and the characters in it. the lcoation. continuity of the shot etc.>>
   - you should do this: 
   **COMPOSITE IMAGES** Sometimes you have more than 3 refrence images. then you have to decide to create in multiple iterations where the last generation will be referenced into the next one. here an example: 
   Our scene says: "it is evening and we are in the warehouse, aladdin is talking to jafar. jafar is holding a dagger. aladdin is holding a sword. aladdin is about to stab jafar."
   now we see that in our library we ditintive artifacts;
   1) Aladdin
   2) Jafar
   3) dagger
   4) Sword
   5) warehouse
   6) setting mood is set.
   We will do the generations one after another. the first generation will feed the second and so on.
   i) get reference image for warehouse. generate new image for warehouse in the evening setting.
   ii) get the refernce image of aladdin and jafar. generate new image for aladdin and jafar in the warehouse in the evening setting.
   ii) generate new image for aladdin holding a sword and jafar holding a dagger.

   so the shot agent will decide to do the following:
   what is the dramatic affect for this shot

   and the continuity agent will decide if the continuity is maintained from last frame last shot and current shot
   the beauty of last frames is that they make the stiched video look more natural and smooth.
   We DONOT have to make each video 7 seconds. we can make it 3-4 seconds if we want to. it has to follow the scene and narrative.
   
7. **Last Frame as Prompt Reference**: << IGNORE THIS. wrong. delete.>>
   - You said "can also prompt using last frame, but won't be perfect match"
   - Does this mean:
     - Option A: Use last frame as **image input** to generation
     - Option B: Use last frame as **reference** but generate new image
     - Option C: Both options available, agent decides?

### Scene Generation Workflow

8. **Scene Start Image Decision Tree**: <<I have explained this above>>
   ```
   Scene Needs Start Image
        ↓
   Shot Agent Evaluates:
     A) Use last frame from previous scene?
     B) Generate new image from character references?
     C) Generate new image from prompt only?
     D) Use last frame as reference in prompt?
   ```
   - What are the decision criteria for A vs B vs C vs D?
   - Cost considerations?
   - Quality/consistency considerations?

9. **Multiple Characters in Scene**:
   - If scene has Jason + Sarah, how are references handled?
   - Retrieve multiple 360° references and composite?
   - Generate scene from scratch with character descriptions?
   - Use Nano Banana to edit/combine references?

10. **Reference Image Updates**:
    - If character appearance changes (costume, injury, aging)?
    - Generate new 360° set?
    - Version the references?
    - How does Brain handle multiple reference versions?

### Verification Agent Workflow 
There are actually two verication steps. 
STEP 1: verification agains our own references << When the image is fully composite  and ready as start image for video, we send it for verification. we eun thru each artifact and ask e.g. is aladdin correctly in this shot. is the sword correctly in the shot?>>
STEP 2: we use env key FAL_VISION_QUERY_MODEL=fal-ai/moondream2/visual-query
Then we ask thing like "are aladding and jafar correctly in this shot? is aladdin holding the sword correctly? is jafar holding the dagger correctly?" and we will get the answer.

**IMMPORTANT** this is the final step of the verification process and step before video production. Image generation is much cheaper than video generation. so we want to ensure that the image is correct before we generate the video.

11. **Verification Query Generation**:
    - When agent writes "Is face of Jackson correct?", what triggers this?
    - Is this automatic for every generated image?
    - Or only when consistency check is needed?
    - What response format does Brain return? (Yes/No? Confidence score? Explanation?)

12. **Verification Failure Handling**:
    - If verification fails (face doesn't match), what happens?
    - Regenerate automatically?
    - Show error to user (stop, costs money)?
    - Try different reference image?
    - How many verification attempts before stopping?

### Cost & Performance
**DO NOT WORRY ABOUT COSTS !!!**
13. **Generation Order Optimization**: <<YES>>
    - Should 360° references be generated all at once (batch) or on-demand?
    - Cost implications of generating all 360° views upfront vs as needed?
    - Should unused reference angles be generated?

14. **Stitching Service Timing**:
    
      - After all scene videos generated? YES
      - Progressive stitching (stitch as scenes complete)? No
      - On-demand when user requests? Yes. this would be a regenration case. 

---

## LOOPHOLES RESOLVED - FINAL ANSWERS

### 1. Composite Image Generation - Complete Flow ✅

**Agent Decision**: Shot agent decides composite flow
**Constraints**:
- Max 3 reference images per request
- As many iterations as required to reach composite shot
- MAX 20 iterations (emergency break)

**Failure Handling**:
- **fal.ai generation fails** → STOP immediately, show error
- **Our verification fails** → Try up to 5 regeneration attempts
- **After 5 failed attempts** → Flag error, STOP entire process

**Webhook Integration**:
- Provide webhook to fal.ai for async image delivery
- Use in conjunction with `tasks.ft.tc` for task orchestration
- Webhook receives generated image → triggers next step

**Example Flow**:
```
Iteration 1: Warehouse evening (1 ref) → Generate → Verify
Iteration 2: Warehouse + Aladdin + Jafar (3 refs total) → Generate → Verify
Iteration 3: Add sword + dagger (2 refs) → Generate → Verify
  ↓ FAILS verification
  Retry 1 → FAILS
  Retry 2 → FAILS
  Retry 3 → FAILS
  Retry 4 → FAILS
  Retry 5 → FAILS
  ↓
FLAG ERROR, STOP PROCESS
```

### 2. Continuity Agent - Defined ✅

**Execution**: Parallel with shot agent
**Responsibilities**: Verify continuity from last frame to current shot

### 3. Two-Step Verification - Orchestration ✅

**Both verifications run in PARALLEL**:
- STEP 1: Verify against our references (Brain multimodal query)
- STEP 2: Visual query model (FAL_VISION_QUERY_MODEL=fal-ai/moondream2/visual-query)

**Pass Criteria**: BOTH must pass to proceed
**Failure**: If either fails, counts toward the 5 retry limit

### 4. Scene Duration Logic - Resolved ✅

**Decision Maker**: Shot agent + Narrative context
**Constraints**:
- Minimum: 3 seconds
- Maximum: 7 seconds
- Story bible specifies expected duration when screenplay broken into scenes

**Purpose**: Important for audio generation (future phase)
**Storage**: Scene metadata in PayloadCMS

### 5. Dramatic Effect Evaluation ✅

**Method**: LLM evaluation of scene context
**Example**: "Aladdin strikes Jafar from top" → Shot agent infers:
- Camera angle: High angle shot
- Lighting: Dramatic shadows
- Pacing: Fast action
- Framing: Close-up on strike

**All decisions stored in**: Scene metadata in PayloadCMS

**IMPORTANT**: All scene-related data lives in PayloadCMS only - single source of truth for editing

### 6. Reference Image Versioning ✅

**Solution**: Name + Description system
**Rule**: EVERY MEDIA must have name and description
- Name: Identifies the artifact (e.g., "Aladdin", "Aladdin_injured")
- Description: Contains all knowledge about the image
  - Appearance details
  - Visible features
  - Context (e.g., "injured after Scene 5 fight")
  - Angle/view
  - Lighting
  - Emotional state
  - Any other relevant details

**Action**: Add name/description if not present during generation

### 7. Reference Selection Logic ✅

**During scene generation**: Reference ALL artifacts needed
**Process**: Agent parses scene description and identifies:
- Characters present
- Props needed
- Location
- Any other visual elements

**Rule**: No surprises - all references must be explicitly identified and retrieved before generation

### 8. 360° Generation Timing - Clarified ✅

**Sequential Flow**:
```
Character Profile Complete
     ↓
Generate Master Reference (single image)
     ↓
Generate 360° Views (can be parallel - 6 images at once)
     ↓
Store all in Brain with descriptions
```

**Only 360° views can be parallel** - Master must complete first

### 9. Department Execution - Sequential ✅

**Correction**: Departments must run SEQUENTIALLY due to dependencies

**Proposed Execution Order**:
```
Phase 1:
- Character Department → Profiles + Master + 360°
- World Department → Story Bible
- Visual Department → Style Guide

Phase 2 (depends on Phase 1):
- Story Department → Screenplay broken into scenes (needs characters, story bible)

Phase 3 (depends on Phase 2):
- Other departments → Their outputs

Phase 4 (final):
- Brain ingestion of all qualified data
```

### 10. Error Display & Handling ✅

**Display**: Show all errors, append if multiple
**Persistence**: User must manually dismiss
**Behavior**: Process normally STOPS when error occurs (no continuation)
**Logging**: Errors shown as global notification at top of all pages

### 11. Prompt Template Storage ✅

**Location**: Create dedicated `Prompts` collection in PayloadCMS

**Structure**:
```typescript
{
  name: string;              // Template name
  description: string;       // What this template is for
  prompt: string;            // Template with variables
  category?: string;         // e.g., "character", "scene", "location"
  applicableTo?: string[];   // Which collections can use this
}
```

**Variable Format**: Square brackets
- Example: `"Generate an image of [character_name] in [location] during [time_of_day]"`
- Agent infers and replaces variables when using template

### 12. Nano Banana Usage - Clarified ✅

**Two Use Cases**:

1. **360° Generation** (one-time per character):
   - Input: Master reference image
   - Model: FAL_IMAGE_TO_IMAGE_MODEL=fal-ai/nano-banana/edit
   - Output: 6 angle views (front, back, left, right, 3/4, full)

2. **Scene Similarity Reference** (optional):
   - Input: Last frame or reference image
   - Purpose: Extract traits/style for new scene
   - Use when: Want SIMILAR scene, not SAME scene
   - Model: Same (fal-ai/nano-banana/edit)

**Not conflicting**: Different purposes, same model

---

## ARCHITECTURE CONFIRMED - NO LOOPHOLES

### Critical Workflows Defined:

✅ **Composite Generation**: Max 3 refs/request, max 20 iterations, 5 retry limit, webhook-based
✅ **Verification**: Parallel 2-step (Brain + Vision Model), both must pass
✅ **Continuity**: Parallel agent checks last-frame consistency
✅ **Duration**: 3-7s range, decided by shot agent + story bible
✅ **Dramatic Effect**: LLM-based scene analysis → camera/lighting/pacing decisions
✅ **Versioning**: Name + Description system for all media
✅ **Reference Selection**: Explicit artifact identification, no surprises
✅ **360° Timing**: Sequential (Profile → Master → 360° parallel)
✅ **Department Order**: Sequential execution with dependency phases
✅ **Error Handling**: Show all, user dismiss, stop on error
✅ **Prompts**: Dedicated collection with variable substitution
✅ **Nano Banana**: Dual purpose (360° + similarity reference)

### Data Flow Summary:

```
Gather DB (unqualified)
     ↓
User: "Generate Qualified Data" button (10s popup)
     ↓
SEQUENTIAL QUALIFICATION:
  Phase 1: Character (360°) + World (Bible) + Visual (Style)
  Phase 2: Story (Screenplay → Scenes with duration)
  Phase 3: Other departments
  Phase 4: Brain ingestion
     ↓
Qualified DB + PayloadCMS (scenes with metadata)
     ↓
MEDIA GENERATION (per scene):
  Shot Agent → Composite iterations (max 20, 3 refs/iter)
       ↓
  Continuity Agent → Check last frame (parallel)
       ↓
  Generate Composite Image (webhook → tasks.ft.tc)
       ↓
  Two-Step Verification (parallel):
    - Brain reference check
    - Vision model query
       ↓
  5 retry attempts if fails
       ↓
  Pass → Video Generation (3-7s)
       ↓
  Extract Last Frame → Next scene
     ↓
After all scenes: Video Stitching Service
     ↓
Final Product
```

**Architecture is complete and loophole-free. Ready to implement.**

---

## Model References & Documentation

**Generation Model Documentation**: See `\docs\model-references\` for LLM and image generation model specifications, API documentation, and usage examples.

**Key Models Used**:
- **Text to Image**: FAL_TEXT_TO_IMAGE_MODEL=fal-ai/nano-banana
- **Image to Image (360°)**: FAL_IMAGE_TO_IMAGE_MODEL=fal-ai/nano-banana/edit
- **Vision Query**: FAL_VISION_QUERY_MODEL=fal-ai/moondream2/visual-query
- **Last Frame Service**: last-frame.ft.tc
- **Video Stitching**: last-frame.ft.tc (video stitching endpoint)
- **Task Orchestration**: tasks.ft.tc

**Additional Documentation**:
- Last frame service usage: `docs/last-frame/how-to-use.md`
- Model references: `docs/model-references/`
