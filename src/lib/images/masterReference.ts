/**
 * Phase 5: Master Reference Image Generation
 * Creates definitive reference images for characters and locations
 */

import { getFalClient } from '../fal/client'
import { getBrainClient } from '../brain/client'
import type {
  MasterReferenceConfig,
  ImageGenerationResult,
  FalGenerateImageRequest,
} from '../fal/types'
import axios from 'axios'

/**
 * Generate master reference image for a character or location
 */
export async function generateMasterReference(
  config: MasterReferenceConfig
): Promise<ImageGenerationResult> {
  const startTime = Date.now()

  try {
    // 1. Query Brain for existing visual specifications
    const brainClient = getBrainClient()
    const existingData = await brainClient.getNode(config.subjectId)

    let enhancedDescription = config.description

    if (existingData?.properties) {
      // Enhance prompt with brain data
      const visualSpecs = existingData.properties.visualSpecs || {}
      enhancedDescription = buildEnhancedPrompt(config.description, visualSpecs)
    }

    // 2. Build FAL.ai generation request
    const falRequest: FalGenerateImageRequest = {
      prompt: enhancedDescription,
      model: config.model || 'fal-ai/flux/dev',
      imageSize: config.resolution || { width: 1024, height: 1024 },
      numImages: 1,
      guidance: 8.5, // Higher guidance for master reference
      steps: 35, // More steps for quality
      format: 'png',
      enableSafetyChecker: true,
    }

    // Add style guide to prompt if provided
    if (config.styleGuide) {
      falRequest.prompt = applyStyleGuide(falRequest.prompt, config.styleGuide)
    }

    // 3. Generate image with FAL.ai
    const falClient = getFalClient()
    const generationStart = Date.now()
    const falResponse = await falClient.generateImage(falRequest)
    const generationTime = Date.now() - generationStart

    if (!falResponse.images || falResponse.images.length === 0) {
      throw new Error('No images generated by FAL.ai')
    }

    const generatedImage = falResponse.images[0]

    // 4. Quality validation
    const qualityScore = await validateImageQuality(
      generatedImage.url,
      config.qualityThreshold || 0.85
    )

    if (qualityScore < (config.qualityThreshold || 0.85)) {
      throw new Error(
        `Generated image quality (${qualityScore.toFixed(2)}) below threshold (${config.qualityThreshold || 0.85})`
      )
    }

    // 5. Upload to R2 storage via Payload
    const uploadStart = Date.now()
    const mediaRecord = await uploadToPayload(
      generatedImage.url,
      {
        filename: `master-ref-${config.subjectId}-${Date.now()}.png`,
        alt: `Master reference for ${config.subjectType} ${config.subjectId}`,
        projectId: config.projectId,
        subjectId: config.subjectId,
        subjectType: config.subjectType,
        referenceType: 'master',
        generationMetadata: {
          prompt: falRequest.prompt,
          model: falRequest.model,
          seed: falResponse.seed,
          qualityScore,
        },
      }
    )
    const uploadTime = Date.now() - uploadStart

    // 6. Store in Brain for future reference
    await brainClient.addNode({
      type: 'concept',
      properties: {
        entityType: 'master-reference',
        subjectId: config.subjectId,
        subjectType: config.subjectType,
        projectId: config.projectId,
        mediaId: mediaRecord.id,
        imageUrl: mediaRecord.url,
        prompt: falRequest.prompt,
        qualityScore,
        visualSpecs: extractVisualSpecs(config.description),
        createdAt: new Date().toISOString(),
      },
    })

    const totalTime = Date.now() - startTime

    return {
      success: true,
      mediaId: mediaRecord.id,
      url: mediaRecord.url,
      width: generatedImage.width,
      height: generatedImage.height,
      seed: falResponse.seed,
      timings: {
        generation: generationTime,
        upload: uploadTime,
        total: totalTime,
      },
      metadata: {
        qualityScore,
        model: falRequest.model,
        prompt: falRequest.prompt,
      },
    }
  } catch (error) {
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error during generation',
    }
  }
}

/**
 * Build enhanced prompt with Brain data
 */
function buildEnhancedPrompt(baseDescription: string, visualSpecs: Record<string, any>): string {
  const enhancements: string[] = [baseDescription]

  if (visualSpecs.eyeColor) enhancements.push(`${visualSpecs.eyeColor} eyes`)
  if (visualSpecs.hairColor) enhancements.push(`${visualSpecs.hairColor} hair`)
  if (visualSpecs.skinTone) enhancements.push(`${visualSpecs.skinTone} skin`)
  if (visualSpecs.height) enhancements.push(`${visualSpecs.height} height`)
  if (visualSpecs.build) enhancements.push(`${visualSpecs.build} build`)

  return enhancements.join(', ')
}

/**
 * Apply style guide to prompt
 */
function applyStyleGuide(
  prompt: string,
  styleGuide: MasterReferenceConfig['styleGuide']
): string {
  const additions: string[] = []

  if (styleGuide?.artStyle) additions.push(styleGuide.artStyle)
  if (styleGuide?.lighting) additions.push(`${styleGuide.lighting} lighting`)
  if (styleGuide?.mood) additions.push(`${styleGuide.mood} mood`)
  if (styleGuide?.colorPalette?.length) {
    additions.push(`color palette: ${styleGuide.colorPalette.join(', ')}`)
  }

  return additions.length > 0 ? `${prompt}, ${additions.join(', ')}` : prompt
}

/**
 * Validate image quality using Brain embeddings
 */
async function validateImageQuality(imageUrl: string, threshold: number): Promise<number> {
  try {
    // In production, this would analyze image quality metrics
    // For now, return high quality score
    // TODO: Integrate with image quality analysis service
    return 0.92
  } catch (error) {
    console.warn('Quality validation failed, defaulting to 0.8:', error)
    return 0.8
  }
}

/**
 * Extract visual specifications from description
 */
function extractVisualSpecs(description: string): Record<string, any> {
  // Basic extraction (in production, use NLP)
  const specs: Record<string, any> = {}

  // Extract color mentions
  const colorRegex = /(brown|blue|green|hazel|amber|gray|black|blonde|red|auburn) (eyes|hair)/gi
  const colorMatches = description.matchAll(colorRegex)
  for (const match of colorMatches) {
    const key = match[2].toLowerCase()
    specs[`${key}Color`] = match[1]
  }

  return specs
}

/**
 * Upload image to Payload Media collection
 */
async function uploadToPayload(
  imageUrl: string,
  metadata: Record<string, any>
): Promise<{ id: string; url: string }> {
  try {
    // Download image from FAL.ai
    const imageResponse = await axios.get(imageUrl, { responseType: 'arraybuffer' })
    const imageBuffer = Buffer.from(imageResponse.data)

    // Upload to Payload API
    const formData = new FormData()
    const blob = new Blob([imageBuffer], { type: 'image/png' })
    formData.append('file', blob, metadata.filename)
    formData.append('alt', metadata.alt)

    // Add all metadata as JSON
    Object.keys(metadata).forEach((key) => {
      if (key !== 'filename' && key !== 'alt' && typeof metadata[key] !== 'object') {
        formData.append(key, metadata[key])
      } else if (typeof metadata[key] === 'object') {
        formData.append(key, JSON.stringify(metadata[key]))
      }
    })

    const payloadUrl = process.env.PAYLOAD_PUBLIC_SERVER_URL || 'http://localhost:3000'
    const uploadResponse = await axios.post(`${payloadUrl}/api/media`, formData, {
      headers: {
        'Content-Type': 'multipart/form-data',
      },
    })

    return {
      id: uploadResponse.data.doc.id,
      url: uploadResponse.data.doc.url,
    }
  } catch (error) {
    throw new Error(
      `Failed to upload to Payload: ${error instanceof Error ? error.message : 'Unknown error'}`
    )
  }
}
